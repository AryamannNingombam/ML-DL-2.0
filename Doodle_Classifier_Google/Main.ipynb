{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import HelperFunctions as HF\n",
    "\n",
    "conversions = {\n",
    "    \"alarm\" : 0,\n",
    "    'bread' : 1,\n",
    "    'eiffel_tower' : 2,\n",
    "    'mona_lisa' : 3,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we would be having four hidden layers,\n",
    "# first would have X number of perceptrons, second 2X,\n",
    "# third 2X, fourth X, this excludes the input and output layer;\n",
    "import HelperFunctions as HF\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, number_of_perceptrons, test_X, test_Y, train_X, train_Y,\n",
    "                 dims, M, M_test):\n",
    "        self.number_of_perceptrons = number_of_perceptrons\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y\n",
    "        self.M_test = M_test\n",
    "        self.dims = dims\n",
    "\n",
    "        # first layer parameters;\n",
    "        self.first_layer_weights = HF.random_initialization(number_of_perceptrons,\n",
    "                                                            dims) * np.sqrt(1/(dims))\n",
    "\n",
    "        self.first_layer_bias = HF.zeros(1,\n",
    "                                         number_of_perceptrons)\n",
    "\n",
    "        self.first_layer_v_w = HF.zeros(number_of_perceptrons,\n",
    "                                        dims)\n",
    "        self.first_layer_s_w = HF.zeros(number_of_perceptrons,\n",
    "                                        dims)\n",
    "        self.first_layer_v_b = HF.zeros(1,\n",
    "                                        number_of_perceptrons)\n",
    "        self.first_layer_s_b = HF.zeros(1,\n",
    "                                        number_of_perceptrons)\n",
    "\n",
    "        # second layer parameters;\n",
    "        self.second_layer_weights = HF.random_initialization(2 * number_of_perceptrons,\n",
    "                                                             number_of_perceptrons) * np.sqrt(1/(number_of_perceptrons))\n",
    "\n",
    "        self.second_layer_bias = HF.zeros(1,\n",
    "                                          2*number_of_perceptrons)\n",
    "\n",
    "        self.second_layer_v_w = HF.zeros(2 * number_of_perceptrons,\n",
    "                                         number_of_perceptrons)\n",
    "        self.second_layer_s_w = HF.zeros(2 * number_of_perceptrons,\n",
    "                                         number_of_perceptrons)\n",
    "\n",
    "        self.second_layer_v_b = HF.zeros(1,\n",
    "                                         2*number_of_perceptrons)\n",
    "        self.second_layer_s_b = HF.zeros(1,\n",
    "                                         2*number_of_perceptrons)\n",
    "\n",
    "        # third layer parameters;\n",
    "        self.third_layer_weights = HF.random_initialization(2*number_of_perceptrons,\n",
    "                                                            2*number_of_perceptrons) * np.sqrt(1/(2*number_of_perceptrons))\n",
    "        self.third_layer_bias = HF.zeros(1,\n",
    "                                         2*number_of_perceptrons)\n",
    "\n",
    "        self.third_layer_v_w = HF.zeros(2*number_of_perceptrons,\n",
    "                                        2*number_of_perceptrons)\n",
    "        self.third_layer_s_w = HF.zeros(2*number_of_perceptrons,\n",
    "                                        2*number_of_perceptrons)\n",
    "\n",
    "        self.third_layer_v_b = HF.zeros(1,\n",
    "                                        2*number_of_perceptrons)\n",
    "        self.third_layer_s_b = HF.zeros(1,\n",
    "                                        2*number_of_perceptrons)\n",
    "\n",
    "        # fourth layer parameters;\n",
    "        self.fourth_layer_weights = HF.random_initialization(\n",
    "            number_of_perceptrons, 2*number_of_perceptrons) * np.sqrt(1/(2*number_of_perceptrons))\n",
    "\n",
    "        self.fourth_layer_bias = HF.zeros(1,\n",
    "                                          number_of_perceptrons)\n",
    "\n",
    "        self.fourth_layer_v_w = HF.zeros(\n",
    "            number_of_perceptrons, 2*number_of_perceptrons)\n",
    "        self.fourth_layer_s_w = HF.zeros(\n",
    "            number_of_perceptrons, 2*number_of_perceptrons)\n",
    "\n",
    "        self.fourth_layer_v_b = HF.zeros(1,\n",
    "                                         number_of_perceptrons)\n",
    "\n",
    "        self.fourth_layer_s_b = HF.zeros(1,\n",
    "                                         number_of_perceptrons)\n",
    "        # output layer parameters;\n",
    "        self.output_layer = HF.random_initialization(\n",
    "            4, number_of_perceptrons) * np.sqrt(2/(number_of_perceptrons))\n",
    "\n",
    "        self.cost_values = []\n",
    "\n",
    "        self.M = M\n",
    "\n",
    "    def forward_prop_train(self):\n",
    "        Z1 = np.matmul(self.train_X, self.first_layer_weights.T) + \\\n",
    "            self.first_layer_bias\n",
    "        A1 = np.tanh(Z1)\n",
    "\n",
    "        Z2 = np.matmul(A1, self.second_layer_weights.T) + \\\n",
    "            self.second_layer_bias\n",
    "        A2 = np.tanh(Z2)\n",
    "\n",
    "        Z3 = np.matmul(A2, self.third_layer_weights.T) + self.third_layer_bias\n",
    "        A3 = np.tanh(Z3)\n",
    "\n",
    "        Z4 = np.matmul(A3, self.fourth_layer_weights.T) + \\\n",
    "            self.fourth_layer_bias\n",
    "        A4 = np.tanh(Z4)\n",
    "\n",
    "        Z_output = np.matmul(A4, self.output_layer.T)\n",
    "        A_output = HF.sigmoid(Z_output)\n",
    "\n",
    "        values = {\n",
    "            'A1': A1,\n",
    "            'A2': A2,\n",
    "            'A3': A3,\n",
    "            'A4': A4,\n",
    "            'A_output': A_output\n",
    "        }\n",
    "\n",
    "        return values\n",
    "\n",
    "    def forward_prop_test(self):\n",
    "        Z1 = np.matmul(self.test_X, self.first_layer_weights.T) + \\\n",
    "            self.first_layer_bias\n",
    "        A1 = np.tanh(Z1)\n",
    "\n",
    "        Z2 = np.matmul(A1, self.second_layer_weights.T) + \\\n",
    "            self.second_layer_bias\n",
    "        A2 = np.tanh(Z2)\n",
    "\n",
    "        Z3 = np.matmul(A2, self.third_layer_weights.T) + self.third_layer_bias\n",
    "        A3 = np.tanh(Z3)\n",
    "\n",
    "        Z4 = np.matmul(A3, self.fourth_layer_weights.T) + \\\n",
    "            self.fourth_layer_bias\n",
    "        A4 = np.tanh(Z4)\n",
    "\n",
    "        Z_output = np.matmul(A4, self.output_layer.T)\n",
    "        temp = HF.sigmoid(Z_output)\n",
    "        print(temp)\n",
    "        A_output = np.argmax(temp, axis=1)\n",
    "\n",
    "        A_output = A_output.reshape(A_output.shape[0], 1)\n",
    "        return A_output\n",
    "\n",
    "    def plot_graph(self):\n",
    "\n",
    "        X_values = np.arange(len(self.cost_values))\n",
    "        Y_values = self.cost_values\n",
    "        plt.plot(X_values, Y_values)\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    def test(self):\n",
    "        result = self.forward_prop_test()\n",
    "        return result\n",
    "\n",
    "    def train(self, number_of_iterations=2000):\n",
    "        for _ in range(number_of_iterations):\n",
    "            values = self.forward_prop_train()\n",
    "            self.backward_prop_gradient_descent(values, iteration=_+1)\n",
    "        self.plot_graph()\n",
    "        self.cost_values = []\n",
    "\n",
    "    def train_adam(self, number_of_iterations=500):\n",
    "        for _ in range(number_of_iterations):\n",
    "            values = self.forward_prop_train()\n",
    "            self.backward_prop_adam_optimizer(values, iteration=_+1)\n",
    "        self.plot_graph()\n",
    "        self.cost_values = []\n",
    "\n",
    "    def calculate_cost(self, value, iteration):\n",
    "        total_cost = np.squeeze(np.sum(np.power(value, 2)))\n",
    "        print(f\"Iteration : {iteration} | {total_cost}\")\n",
    "        self.cost_values.append(total_cost)\n",
    "\n",
    "    def backward_prop_adam_optimizer(self, values, iteration, beta1=0.9, beta2=0.999, epsilon=1e-8, gamma=0.001):\n",
    "        A1 = values['A1']\n",
    "        A2 = values['A2']\n",
    "        A3 = values['A3']\n",
    "        A4 = values['A4']\n",
    "        A_output = values['A_output']\n",
    "\n",
    "        assert(A_output.shape == self.train_Y.shape)\n",
    "\n",
    "        output_layer_error = A_output - self.train_Y\n",
    "        self.calculate_cost(output_layer_error, iteration)\n",
    "        # doing calculations for the output layer;\n",
    "        dW_output = np.matmul(output_layer_error.T, A4)\n",
    "\n",
    "        assert(dW_output.shape == (4, self.number_of_perceptrons))\n",
    "        self.output_layer = self.output_layer + \\\n",
    "            (1/self.M) * gamma * dW_output\n",
    "\n",
    "        # doing calculations for the fourth layer;\n",
    "        error__fourth_layer = np.matmul(\n",
    "            output_layer_error, self.output_layer) * HF.get_tanh_derivative(A4)\n",
    "        dW_fourth_layer = np.matmul(error__fourth_layer.T, A3)\n",
    "        assert(dW_fourth_layer.shape == self.fourth_layer_weights.shape)\n",
    "\n",
    "        self.fourth_layer_v_w = beta1 * \\\n",
    "            self.fourth_layer_v_w + (1-beta1) * dW_fourth_layer\n",
    "        self.fourth_layer_s_w = beta2 * self.fourth_layer_s_w + \\\n",
    "            (1-beta2) * np.power(dW_fourth_layer, 2)\n",
    "\n",
    "        temp = np.sum(error__fourth_layer, axis=0, keepdims=True)\n",
    "\n",
    "        self.fourth_layer_v_b = beta1 * self.fourth_layer_v_b + \\\n",
    "            (1-beta2) * temp\n",
    "        self.fourth_layer_s_b = beta2 * self.fourth_layer_s_b + \\\n",
    "            (1-beta2) * np.power(temp, 2)\n",
    "\n",
    "\n",
    "\n",
    "        self.fourth_layer_weights = self.fourth_layer_weights - \\\n",
    "            (gamma) * (self.fourth_layer_v_w /\n",
    "                       (np.sqrt(self.fourth_layer_s_w) + epsilon))\n",
    "        self.fourth_layer_bias = self.fourth_layer_bias - \\\n",
    "            (gamma) * (self.fourth_layer_v_b /\n",
    "                       (np.sqrt(self.fourth_layer_s_b) + epsilon))\n",
    "\n",
    "        # doing calculations for the third layer;\n",
    "        error_third_layer = np.matmul(\n",
    "            error__fourth_layer, self.fourth_layer_weights\n",
    "        ) * HF.get_tanh_derivative(A3)\n",
    "        dW_third_layer = np.matmul(error_third_layer.T, A2)\n",
    "        assert(dW_third_layer.shape == self.third_layer_weights.shape)\n",
    "\n",
    "        self.third_layer_v_w = beta1 * \\\n",
    "            self.third_layer_v_w + (1-beta1) * dW_third_layer\n",
    "        self.third_layer_s_w = beta2 * self.third_layer_s_w + \\\n",
    "            (1-beta2) * np.power(dW_third_layer, 2)\n",
    "\n",
    "        temp = np.sum(error_third_layer, axis=0, keepdims=True)\n",
    "        self.third_layer_v_b = beta1 * self.third_layer_v_b + \\\n",
    "            (1-beta2) * temp\n",
    "        self.third_layer_s_b = beta2 * self.third_layer_s_b + \\\n",
    "            (1-beta2) * np.power(temp, 2)\n",
    "\n",
    "\n",
    "        self.third_layer_weights = self.third_layer_weights - \\\n",
    "            (gamma) * (self.third_layer_v_w /\n",
    "                       (np.sqrt(self.third_layer_s_w) + epsilon))\n",
    "\n",
    "\n",
    "        self.third_layer_bias = self.third_layer_bias - \\\n",
    "            (gamma) * (self.third_layer_v_b /\n",
    "                       (np.sqrt(self.third_layer_s_b) + epsilon))\n",
    "\n",
    "        # doing calculations for the second layer;\n",
    "        error_second_layer = np.matmul(\n",
    "            error_third_layer, self.third_layer_weights) * HF.get_tanh_derivative(A2)\n",
    "        dW_second_layer = np.matmul(error_second_layer.T, A1)\n",
    "        assert(dW_second_layer.shape == self.second_layer_weights.shape)\n",
    "\n",
    "        self.second_layer_v_w = beta1 * \\\n",
    "            self.second_layer_v_w + (1-beta1) * dW_second_layer\n",
    "        self.second_layer_s_w = beta2 * self.second_layer_s_w + \\\n",
    "            (1-beta2) * np.power(dW_second_layer, 2)\n",
    "\n",
    "\n",
    "        temp = np.sum(error_second_layer, axis=0, keepdims=True)\n",
    "        self.second_layer_v_b = beta1 * self.second_layer_v_b + \\\n",
    "            (1-beta1)*(temp)\n",
    "\n",
    "        self.second_layer_s_b = beta2 * self.second_layer_s_b + \\\n",
    "            (1-beta2) * (np.power(temp, 2))\n",
    "\n",
    "        self.second_layer_weights = self.second_layer_weights - \\\n",
    "            (gamma) * (self.second_layer_v_w /\n",
    "                       (np.sqrt(self.second_layer_s_w) + epsilon))\n",
    "\n",
    "        self.second_layer_bias = self.second_layer_bias - \\\n",
    "            (gamma) * (self.second_layer_v_b /\n",
    "                       (np.sqrt(self.second_layer_s_b) + epsilon))\n",
    "\n",
    "        # doing calculations for the first layer\n",
    "        error_first_layer = np.matmul(\n",
    "            error_second_layer, self.second_layer_weights) * HF.get_tanh_derivative(A1)\n",
    "        dW_first_layer = np.matmul(error_first_layer.T, self.train_X)\n",
    "        assert(dW_first_layer.shape == self.first_layer_weights.shape)\n",
    "\n",
    "        self.first_layer_v_w = beta1 * \\\n",
    "            self.first_layer_v_w + (1-beta1) * dW_first_layer\n",
    "        self.first_layer_s_w = beta2 * self.first_layer_s_w + \\\n",
    "            (1-beta2) * np.power(dW_first_layer, 2)\n",
    "        temp = np.sum(error_first_layer, axis=0, keepdims=True)\n",
    "        self.first_layer_v_b = beta1 * self.first_layer_v_b + \\\n",
    "            (1-beta1)*(temp)\n",
    "        self.first_layer_s_b = beta2 * self.first_layer_s_b + \\\n",
    "            (1-beta2) * (np.power(temp, 2))\n",
    "\n",
    "        self.first_layer_weights = self.first_layer_weights - \\\n",
    "            (gamma) * (self.first_layer_v_w /\n",
    "                       (np.sqrt(self.first_layer_s_w) + epsilon))\n",
    "        self.first_layer_bias = self.first_layer_bias - \\\n",
    "            (gamma) * (self.first_layer_v_b /\n",
    "                       (np.sqrt(self.first_layer_s_b) + epsilon))\n",
    "\n",
    "    def backward_prop_gradient_descent(self, values, iteration, learning_rate=0.003):\n",
    "        A1 = values['A1']\n",
    "        A2 = values['A2']\n",
    "        A3 = values['A3']\n",
    "        A4 = values['A4']\n",
    "        A_output = values['A_output']\n",
    "\n",
    "        assert(A_output.shape == self.train_Y.shape)\n",
    "\n",
    "        output_layer_error = A_output - self.train_Y\n",
    "        self.calculate_cost(output_layer_error, iteration)\n",
    "        # doing calculations for the output layer;\n",
    "        dW_output = np.matmul(output_layer_error.T, A4)\n",
    "\n",
    "        assert(dW_output.shape == (4, self.number_of_perceptrons))\n",
    "\n",
    "        self.output_layer = self.output_layer + \\\n",
    "            (1/self.M) * learning_rate * dW_output\n",
    "\n",
    "        # doing calculations for the fourth layer;\n",
    "        error__fourth_layer = np.matmul(\n",
    "            output_layer_error, self.output_layer) * HF.get_tanh_derivative(A4)\n",
    "        dW_fourth_layer = np.matmul(error__fourth_layer.T, A3)\n",
    "        assert(dW_fourth_layer.shape == self.fourth_layer_weights.shape)\n",
    "\n",
    "        self.fourth_layer_weights = self.fourth_layer_weights - \\\n",
    "            (1/self.M) * learning_rate * dW_fourth_layer\n",
    "\n",
    "        self.fourth_layer_bias = self.fourth_layer_bias - \\\n",
    "            (1/self.M) * learning_rate * \\\n",
    "            np.sum(error__fourth_layer, axis=0, keepdims=True)\n",
    "\n",
    "        # doing calculations for the third layer;\n",
    "        error_third_layer = np.matmul(\n",
    "            error__fourth_layer, self.fourth_layer_weights\n",
    "        ) * HF.get_tanh_derivative(A3)\n",
    "        dW_third_layer = np.matmul(error_third_layer.T, A2)\n",
    "        assert(dW_third_layer.shape == self.third_layer_weights.shape)\n",
    "\n",
    "        self.third_layer_weights = self.third_layer_weights - \\\n",
    "            (1/self.M) * learning_rate * dW_third_layer\n",
    "\n",
    "        self.third_layer_bias = self.third_layer_bias - \\\n",
    "            (1/self.M) * learning_rate * \\\n",
    "            np.sum(error_third_layer, axis=0, keepdims=True)\n",
    "\n",
    "        # doing calculations for the second layer;\n",
    "        error_second_layer = np.matmul(\n",
    "            error_third_layer, self.third_layer_weights) * HF.get_tanh_derivative(A2)\n",
    "        dW_second_layer = np.matmul(error_second_layer.T, A1)\n",
    "        assert(dW_second_layer.shape == self.second_layer_weights.shape)\n",
    "\n",
    "        self.second_layer_weights = self.second_layer_weights - \\\n",
    "            (1/self.M) * learning_rate * dW_second_layer\n",
    "        self.second_layer_bias = self.second_layer_bias - \\\n",
    "            (1/self.M) * learning_rate * \\\n",
    "            np.sum(error_second_layer, axis=0, keepdims=True)\n",
    "\n",
    "        # doing calculations for the first layer\n",
    "        error_first_layer = np.matmul(\n",
    "            error_second_layer, self.second_layer_weights) * HF.get_tanh_derivative(A1)\n",
    "        dW_first_layer = np.matmul(error_first_layer.T, self.train_X)\n",
    "        assert(dW_first_layer.shape == self.first_layer_weights.shape)\n",
    "\n",
    "        self.first_layer_weights = self.first_layer_weights - \\\n",
    "            (1/self.M) * learning_rate * dW_first_layer\n",
    "        self.first_layer_bias = self.first_layer_bias - \\\n",
    "            (1/self.M) * learning_rate * \\\n",
    "            np.sum(error_first_layer, axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Shapes --- Alarm : (123399, 784)  || Bread : (120570, 784) || Eiffel Tower : (134801, 784) || Mona Lise : (121383, 784)\n"
     ]
    }
   ],
   "source": [
    "# loading the datasets;\n",
    "print(\"Loading the dataset...\")\n",
    "alarm_dataset_pre = np.load('./dataset/alarm_clock.npy')\n",
    "bread_dataset_pre = np.load('./dataset/bread.npy')\n",
    "eiffel_tower_dataset_pre = np.load('./dataset/Eiffel_Tower.npy')\n",
    "mona_lisa_dataset_pre = np.load('./dataset/The_Mona_Lisa.npy')\n",
    "print(f\"Shapes --- Alarm : {alarm_dataset_pre.shape}  || Bread : {bread_dataset_pre.shape} || Eiffel Tower : {eiffel_tower_dataset_pre.shape} || Mona Lise : {mona_lisa_dataset_pre.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing the shapes....\n",
      "Shapes --- Alarm : (123399, 785)  || Bread : (120570, 785) || Eiffel Tower : (134801, 785) || Mona Lise : (121383, 785)\n"
     ]
    }
   ],
   "source": [
    "# flattening it for training\n",
    "print(\"Resizing the shapes....\")\n",
    "alarm_dataset_final = HF.add_to_end(HF.normalize(alarm_dataset_pre),0)\n",
    "bread_dataset_final = HF.add_to_end(HF.normalize(bread_dataset_pre),1)\n",
    "eiffel_tower_dataset_final = HF.add_to_end(HF.normalize(eiffel_tower_dataset_pre),2)\n",
    "mona_lisa_dataset_final = HF.add_to_end(HF.normalize(mona_lisa_dataset_pre),3)\n",
    "print(f\"Shapes --- Alarm : {alarm_dataset_final.shape}  || Bread : {bread_dataset_final.shape} || Eiffel Tower : {eiffel_tower_dataset_final.shape} || Mona Lise : {mona_lisa_dataset_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining the arrays and shuffling them....\n",
      "(500153, 785)\n"
     ]
    }
   ],
   "source": [
    "# combining the arrays for training;\n",
    "print(\"Combining the arrays and shuffling them....\")\n",
    "full_dataset = HF.return_full_dataset((alarm_dataset_final,bread_dataset_final,eiffel_tower_dataset_final,mona_lisa_dataset_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500153, 785)\n"
     ]
    }
   ],
   "source": [
    "print(full_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450137, 784) (450137, 1)\n",
      "(50016, 784) (50016, 1)\n"
     ]
    }
   ],
   "source": [
    "[train_data,test_data] = HF.split_dataset(full_dataset)\n",
    "[train_data_X,train_data_Y] = HF.get_X_Y(train_data)\n",
    "[test_data_X,test_data_Y] = HF.get_X_Y(test_data)\n",
    "# temp_test = np.zeros((test_data_Y.shape[0],4))\n",
    "# for i in range(test_data_Y.shape[0]):\n",
    "#     temp_test[i][int(test_data_Y[i])] = 1\n",
    "# test_data_Y = temp_test\n",
    "\n",
    "# temp_train = np.zeros((train_data_Y.shape[0],4))\n",
    "\n",
    "# for i in range(train_data_Y.shape[0]):\n",
    "#     temp_train[i][int(train_data_Y[i])] = 1\n",
    "    \n",
    "# train_data_Y = temp_train\n",
    "    \n",
    "\n",
    "print(train_data_X.shape,train_data_Y.shape)\n",
    "\n",
    "print(test_data_X.shape,test_data_Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "14067/14067 [==============================] - 28s 2ms/step - loss: 0.1855 - accuracy: 0.9384\n",
      "Epoch 2/20\n",
      "14067/14067 [==============================] - 36s 3ms/step - loss: 0.1461 - accuracy: 0.9523\n",
      "Epoch 3/20\n",
      "14067/14067 [==============================] - 30s 2ms/step - loss: 0.1310 - accuracy: 0.9570\n",
      "Epoch 4/20\n",
      "14067/14067 [==============================] - 28s 2ms/step - loss: 0.1198 - accuracy: 0.9605\n",
      "Epoch 5/20\n",
      "14067/14067 [==============================] - 28s 2ms/step - loss: 0.1097 - accuracy: 0.9638\n",
      "Epoch 6/20\n",
      "14067/14067 [==============================] - 28s 2ms/step - loss: 0.1016 - accuracy: 0.9664\n",
      "Epoch 7/20\n",
      "14067/14067 [==============================] - 28s 2ms/step - loss: 0.0942 - accuracy: 0.9686\n",
      "Epoch 8/20\n",
      "14067/14067 [==============================] - 28s 2ms/step - loss: 0.0877 - accuracy: 0.9707\n",
      "Epoch 9/20\n",
      "14067/14067 [==============================] - 30s 2ms/step - loss: 0.0822 - accuracy: 0.9726\n",
      "Epoch 10/20\n",
      "14067/14067 [==============================] - 29s 2ms/step - loss: 0.0771 - accuracy: 0.9739\n",
      "Epoch 11/20\n",
      "14067/14067 [==============================] - 29s 2ms/step - loss: 0.0728 - accuracy: 0.9755\n",
      "Epoch 12/20\n",
      "14067/14067 [==============================] - 29s 2ms/step - loss: 0.0689 - accuracy: 0.9769\n",
      "Epoch 13/20\n",
      "14067/14067 [==============================] - 29s 2ms/step - loss: 0.0656 - accuracy: 0.9778\n",
      "Epoch 14/20\n",
      "14067/14067 [==============================] - 31s 2ms/step - loss: 0.0625 - accuracy: 0.9789\n",
      "Epoch 15/20\n",
      "14067/14067 [==============================] - 29s 2ms/step - loss: 0.0593 - accuracy: 0.9801\n",
      "Epoch 16/20\n",
      "14067/14067 [==============================] - 29s 2ms/step - loss: 0.0569 - accuracy: 0.9808\n",
      "Epoch 17/20\n",
      "14067/14067 [==============================] - 30s 2ms/step - loss: 0.0547 - accuracy: 0.9815\n",
      "Epoch 18/20\n",
      "14067/14067 [==============================] - 29s 2ms/step - loss: 0.0524 - accuracy: 0.9824\n",
      "Epoch 19/20\n",
      "14067/14067 [==============================] - 29s 2ms/step - loss: 0.0513 - accuracy: 0.9826\n",
      "Epoch 20/20\n",
      "14067/14067 [==============================] - 29s 2ms/step - loss: 0.0491 - accuracy: 0.9833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15794e4f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(784,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(10,activation='relu')\n",
    "])\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',loss=loss_fn,metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data_X,train_data_Y,epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 2s 880us/step - loss: 0.2394 - accuracy: 0.9548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2394145429134369, 0.9547744989395142]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data_X,test_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"All ready for the fun(not) part baby!\")\n",
    "print(\"Getting the neural network ready...\")\n",
    "DoodleNeuralNetwork = NeuralNetwork(30, test_data_X, test_data_Y, train_data_X, train_data_Y,\n",
    "                 784, train_data_Y.shape[0],test_data_Y.shape[0])\n",
    "\n",
    "DoodleNeuralNetwork.train_adam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = DoodleNeuralNetwork.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.shape)\n",
    "\n",
    "result2 = np.argmax(test_data_Y,axis=1)\n",
    "result2 = result2.reshape(result2.shape[0],1)\n",
    "\n",
    "test = (result == result2).astype(int)\n",
    "total_examples = test.shape[0]\n",
    "print(test.shape)\n",
    "right_ones = np.squeeze(np.sum(test))\n",
    "print(right_ones)\n",
    "print(total_examples)\n",
    "\n",
    "print(f\"Accuracy : {right_ones/total_examples} \")\n",
    "print(\"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
